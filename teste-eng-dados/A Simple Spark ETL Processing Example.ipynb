{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprender Spark na Prática\n",
    "## Você foi contratado como Engenheiro de Dados na Empresa AllWeDoWithSpark e sua primeira tarefa será executar as seguintes atividades em um sample de um dataset que foi disponibilizado pelo cliente:\n",
    "- Conversão do formato dos arquivos: Converter o arquivo CSV presente no diretório data/input/users/load.csv, para um formato colunar de alta performance de leitura de sua escolha. Justificar brevemente a escolha do formato;\n",
    "- Deduplicação dos dados convertidos: No conjunto de dados convertidos haverão múltiplas entradas para um mesmo registro, variando apenas os valores de alguns dos campos entre elas. Será necessário realizar um processo de deduplicação destes dados, a fim de apenas manter a última entrada de cada registro, usando como referência o id para identificação dos registros duplicados e a data de atualização (update_date) para definição do registro mais recente;\n",
    "- Conversão do tipo dos dados deduplicados: No diretório config haverá um arquivo JSON de configuração (types_mapping.json), contendo os nomes dos campos e os respectivos tipos desejados de output. Utilizando esse arquivo como input, realizar um processo de conversão dos tipos dos campos descritos, no conjunto de dados deduplicados;\n",
    "\n",
    "### Para esta atividade, está sendo utilizada uma Data Science Virtual Machine no MS Azure. Leia mais sobre ela em https://docs.microsoft.com/en-us/azure/machine-learning/data-science-virtual-machine/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando todos os imports necessários\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rank\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo os caminhos\n",
    "SOURCE_FILE = r'C:\\Users\\paulo\\Downloads\\teste-eng-dados\\data\\input\\users\\load.csv'\n",
    "WRITE_DIR = r'C:\\Users\\paulo\\Downloads\\teste-eng-dados\\data\\output'\n",
    "CONFIG_FILE =  r'C:\\Users\\paulo\\Downloads\\teste-eng-dados\\config\\types_mapping.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>address</th>\n",
       "      <th>age</th>\n",
       "      <th>create_date</th>\n",
       "      <th>update_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>david.lynch@cognitivo.ai</td>\n",
       "      <td>David Lynch</td>\n",
       "      <td>(11) 99999-9997</td>\n",
       "      <td>Mulholland Drive, Los Angeles, CA, US</td>\n",
       "      <td>72</td>\n",
       "      <td>2018-03-03 18:47:01.954752</td>\n",
       "      <td>2018-03-03 18:47:01.954752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>david.lynch@cognitivo.ai</td>\n",
       "      <td>David Lynch</td>\n",
       "      <td>(11) 99999-9998</td>\n",
       "      <td>Mulholland Drive, Los Angeles, CA, US</td>\n",
       "      <td>72</td>\n",
       "      <td>2018-03-03 18:47:01.954752</td>\n",
       "      <td>2018-04-14 17:09:48.558151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sherlock.holmes@cognitivo.ai</td>\n",
       "      <td>Sherlock Holmes</td>\n",
       "      <td>(11) 94815-1623</td>\n",
       "      <td>221B Baker Street, London, UK</td>\n",
       "      <td>34</td>\n",
       "      <td>2018-04-21 20:21:24.364752</td>\n",
       "      <td>2018-04-21 20:21:24.364752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>spongebob.squarepants@cognitivo.ai</td>\n",
       "      <td>Spongebob Squarepants</td>\n",
       "      <td>(11) 91234-5678</td>\n",
       "      <td>124 Conch Street, Bikini Bottom, Pacific Ocean</td>\n",
       "      <td>13</td>\n",
       "      <td>2018-05-19 04:07:06.854752</td>\n",
       "      <td>2018-05-19 04:07:06.854752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>david.lynch@cognitivo.ai</td>\n",
       "      <td>David Lynch</td>\n",
       "      <td>(11) 99999-9999</td>\n",
       "      <td>Mulholland Drive, Los Angeles, CA, US</td>\n",
       "      <td>72</td>\n",
       "      <td>2018-03-03 18:47:01.954752</td>\n",
       "      <td>2018-05-23 10:13:59.594752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                name                  email  \\\n",
       "0   1            david.lynch@cognitivo.ai            David Lynch   \n",
       "1   1            david.lynch@cognitivo.ai            David Lynch   \n",
       "2   2        sherlock.holmes@cognitivo.ai        Sherlock Holmes   \n",
       "3   3  spongebob.squarepants@cognitivo.ai  Spongebob Squarepants   \n",
       "4   1            david.lynch@cognitivo.ai            David Lynch   \n",
       "\n",
       "             phone                                         address  age  \\\n",
       "0  (11) 99999-9997           Mulholland Drive, Los Angeles, CA, US   72   \n",
       "1  (11) 99999-9998           Mulholland Drive, Los Angeles, CA, US   72   \n",
       "2  (11) 94815-1623                   221B Baker Street, London, UK   34   \n",
       "3  (11) 91234-5678  124 Conch Street, Bikini Bottom, Pacific Ocean   13   \n",
       "4  (11) 99999-9999           Mulholland Drive, Los Angeles, CA, US   72   \n",
       "\n",
       "                  create_date                 update_date  \n",
       "0  2018-03-03 18:47:01.954752  2018-03-03 18:47:01.954752  \n",
       "1  2018-03-03 18:47:01.954752  2018-04-14 17:09:48.558151  \n",
       "2  2018-04-21 20:21:24.364752  2018-04-21 20:21:24.364752  \n",
       "3  2018-05-19 04:07:06.854752  2018-05-19 04:07:06.854752  \n",
       "4  2018-03-03 18:47:01.954752  2018-05-23 10:13:59.594752  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explorando o arquivo de origem com Pandas\n",
    "pd_sourcefile = pd.read_csv(SOURCE_FILE, sep=',')\n",
    "pd_sourcefile.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dado os requisitos, realizaremos o processamento ETL com Apache Spark na seguinte ordem:\n",
    "1. Carregamento do arquivo para um dataframe do Spark\n",
    "2. Deduplicação dos dados\n",
    "3. Conversão dos tipos de dados a partir dos tipos sugeridos no arquivo de configuração\n",
    "4. Escrita do arquivo do diretório de origem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construindo uma Sessão Spark e Executando a Carga do Arquivo de Origem:\n",
    "spark = SparkSession.builder.appName(\"A Simple Spark ETL Processing Example\").enableHiveSupport().getOrCreate()\n",
    "df_sourcefile = spark.read.format(\"csv\").option(\"header\", \"true\").load(SOURCE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desduplicando os Dados no Dataframe utilizando Window Functions\n",
    "windowSpec = Window.partitionBy(df_sourcefile['id']).orderBy(df_sourcefile['update_date'].desc())\n",
    "df_deduped = df_sourcefile.withColumn(\"rank_id\", rank().over(windowSpec)).filter(col('rank_id') == 1).drop(col(\"rank_id\")).sort(col('id').asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "| id|                name|               email|          phone|             address|age|         create_date|         update_date|\n",
      "+---+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "|  1|david.lynch@cogni...|         David Lynch|(11) 99999-9999|Mulholland Drive,...| 72|2018-03-03 18:47:...|2018-05-23 10:13:...|\n",
      "|  2|sherlock.holmes@c...|     Sherlock Holmes|(11) 94815-1623|221B Baker Street...| 34|2018-04-21 20:21:...|2018-04-21 20:21:...|\n",
      "|  3|spongebob.squarep...|Spongebob Squarep...|(11) 98765-4321|122 Conch Street,...| 13|2018-05-19 04:07:...|2018-05-19 05:08:...|\n",
      "+---+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificando os dados desduplicados\n",
    "df_deduped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- create_date: string (nullable = true)\n",
      " |-- update_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convertendo os tipos de dados baseado no arquivo de configuração\n",
    "# Primeiro, vamos dar uma olhada no schema atual\n",
    "df_deduped.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checando o arquivo de configuração, devemos alterar três colunas: age, create_date e update_date\n",
    "def cast_from_file(json_file, df):\n",
    "    with open(json_file) as types_mapping:\n",
    "        data = json.load(types_mapping)\n",
    "        for d in data:\n",
    "            df = df.withColumn(d,col(d).cast(data[d]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando a função definida para conversão\n",
    "df_converted = cast_from_file(CONFIG_FILE, df_deduped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcional: considerando o dataset, vamos adicionalmente converter a coluna id\n",
    "df_converted = df_converted.withColumn('id', col('id').cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- create_date: timestamp (nullable = true)\n",
      " |-- update_date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validando o schema após a função\n",
    "df_converted.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O Parquet foi escolhido como formato de saída porque, além de ser um formato colunar de alta performance de leitura, é otimizado para o Spark, comumente adotado quando se precisa armazenar terabytes de dados em um Data Lake e trabalha melhor a soluções como AWS Athena, Google BigQuery etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escrevendo no diretório de saída no formato Parquet. Opcionalmente pode se adotar no futuro o partitionBy\n",
    "df_converted.coalesce(1).write.mode(\"overwrite\").parquet(WRITE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
